# kafka地址与端口号 e: e.x:172.18.18.100:9092,172.18.18.101:9092,172.18.18.102:9092
kafka.bootstrap.servers=172.18.18.105:9092,172.18.18.106:9092,172.18.18.107:9092
kafka.request.required.acks=-1
kafka.maximum.time=1000
kafka.retries=0
kafka.key.serializer=org.apache.kafka.common.serialization.StringSerializer
kafka.value.serializer=org.apache.kafka.common.serialization.StringSerializer
kafka.topic=test2
kafka.group.id=group222

es.cluster.name=hbase2es-cluster
es.hosts=172.18.18.105
es.cluster.port=9300

cluster.dir=/opt/hzgc/bigdata
#facecompare.dir=/opt/hzgc/compare

#zookeeper连接地址与端口 e: e.x:172.18.18.100:2181,172.18.18.101:2181,172.18.18.102:2181
zookeeper.address=172.18.18.105:2181,172.18.18.106:2181,172.18.18.107:2181

cluster.name=face_compare_cluster
#worker参数配置
worker.buffer.size.max=500
#内存中缓存数据的最大值
worker.cach.size.max=5000000
#内存数据的检查时间间隔
worker.memory.check.time=1800000
#内存中记录的过期时间
work.record.time.out=90
#检查任务列表的时间间隔
work.check_task.time=1000
#文件的过期时间
#worker.file.time.out=
#文件检查时间间隔
worker.file.check.time=1800000
#写HBase任务的时间间隔
worker.hbase.write.time=2000
#文件保存路径
worker.file.path=test
#文件保存大小
worker.file.size=134217728
#文件流过期时间
worker.stream.time.out=2
#数据持久化的文件系统 0 本地  1 HDFS
worker.file.save.system=0

worker.address=localhost
tasktracker.group=compareTask
worker.readfiles_per_thread=1
worker.executors.to.compare=10
worker.executors.to.loadfile=15
#持久化触发方式 0 定期触发  1定量触发
worker.flush.program=0

#是否删除过期文件
delete.open=1
