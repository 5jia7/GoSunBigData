version: '2.3'

services:
  collect:
    image: ${DOCKER_REPOSITOR}/hzgc/collect:${COLLECT_VERSION}
    container_name: collect
    restart: always
    environment:
    - EUREKA_IP=${EUREKA_HOST_IP}
    - EUREKA_PORT=${EUREKA_HOST_PORT}
    - ZOOKEEPER_HOST=${ZK_HOST}:${ZK_PORT}
    network_mode: "host"
    extra_hosts:
    - platform:${PLATFORM_HOST}
    volumes:
    - ${DOCKER_HOME}/collect/log:/log

  kafka-parquet:
    image: ${DOCKER_REPOSITOR}/hzgc/spark:${SPARK_VERSION}
    container_name: kafka-parquet
    command: /start-kafka-to-parquet.sh
    restart: always
    network_mode: "host"
    environment:
    - KAFKA_BROKER=${KAFKA_HOST}:${KAFKA_PORT}
    - ZK_ADDRESS=${ZK_HOST}:${ZK_PORT}
    - ES_NODE=${ES_HOST}
    extra_hosts:
    - platform:${PLATFORM_HOST}
    - ${DOCKER_HOST_NAME}:${DOCKER_HOST_IP}
    volumes:
#    - ${DOCKER_HOME}/spark/sparkJob.properties:/usr/spark-2.2.0/conf/sparkJob.properties
    - ${DOCKER_HOME}/spark/parquet:/parquet

 # kafka-tidb:
 #   image: ${DOCKER_REPOSITOR}/hzgc/spark:${VERSION}
 #   container_name: kafka-tidb
 #   command: /start-kafka-to-tidb.sh
 #   restart: always
 #   network_mode: "host"
 #   extra_hosts:
 #   - platform:${PLATFORM_HOST}
 #   - ${DOCKER_HOST_NAME}:${DOCKER_HOST_IP}
 #   environment:
 #   - JDBC_IP=${MYSQL_HOST}
 #   - ZK_ADDRESS=${ZK_HOST}:${ZK_PORT}
 #   - KAFKA_BROKER=${KAFKA_HOST}:${KAFKA_PORT}
 #   - JDBC_PORT=${MYSQL_PORT}

  facecompare:
    image: ${DOCKER_REPOSITOR}/hzgc/facecompare:${FACECOMPARE_VERSION}
    container_name: facecompare
    restart: always
    environment:
    - ZK_ADDRESS=${ZK_HOST}:${ZK_PORT}
    - ES_HOST=${ES_HOST}
    - KAFKA_SERVERS=${KAFKA_HOST}:${KAFKA_PORT}
    network_mode: "host"
    volumes:
    - ${DOCKER_HOME}/facecompare/log:/worker/log
    - /opt/GsFaceLib:/opt/GsFaceLib
    - ${DOCKER_HOME}/facecompare/data:/worker/matedata